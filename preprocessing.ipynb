{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nibabel nilearn scikit-learn pandas numpy matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn.masking import intersect_masks\n",
    "from nilearn.image import resample_to_img, load_img, math_img\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PsychPY timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert start/stop times to scan indices\n",
    "def time_to_index(times, TR=1.5):\n",
    "    return np.round(np.array(times) / TR).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_length_tr_range(start_times, TR=1.5, duration_sec=10):\n",
    "    \"\"\"\n",
    "    Convert a list of start times to TR indices of fixed duration.\n",
    "    Returns a list of lists (one TR range per event).\n",
    "    \"\"\"\n",
    "    n_TRs = int(np.round(duration_sec / TR))  # e.g. 10s / 1.5s = 6.67 → 7\n",
    "    indices = []\n",
    "    for start in start_times:\n",
    "        start_idx = int(np.round(start / TR))\n",
    "        indices.append(list(range(start_idx, start_idx + n_TRs)))\n",
    "    return indices\n",
    "    \n",
    "def convert_psychopy_time_to_fmri_index(beh_file, subject_id, TR=1.5):\n",
    "    df = pd.read_csv(beh_file)\n",
    "    picture_col = f'picture_{subject_id}'\n",
    "\n",
    "    image_to_view_TRs = {}\n",
    "    image_to_recall_TRs = {}\n",
    "    imagine_category_blocks = defaultdict(list)  # 'dog' → list of TR lists, etc.\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        image_name = row.get(picture_col)\n",
    "        if pd.isna(image_name):\n",
    "            continue\n",
    "\n",
    "        # VIEW (fixed length)\n",
    "        if not pd.isna(row.get('view.started')):\n",
    "            tr_range = get_fixed_length_tr_range([row['view.started']], TR)[0]\n",
    "            image_to_view_TRs[image_name] = tr_range\n",
    "\n",
    "        # RECALL (fixed length)\n",
    "        if not pd.isna(row.get('recall.started')):\n",
    "            tr_range = get_fixed_length_tr_range([row['recall.started']], TR)[0]\n",
    "            image_to_recall_TRs[image_name] = tr_range\n",
    "\n",
    "    # IMAGINE (grouped by category)\n",
    "    if 'imagine_task.started' in df.columns:\n",
    "        valid_rows = df[df['imagine_task.started'].notna()]\n",
    "        for _, row in valid_rows.iterrows():\n",
    "            image_name = row.get(picture_col)\n",
    "            if pd.isna(image_name):\n",
    "                continue\n",
    "\n",
    "            # Infer category\n",
    "            if 'dog' in image_name.lower():\n",
    "                category = 'dog'\n",
    "            elif 'sunflower' in image_name.lower():\n",
    "                category = 'sunflower'\n",
    "            else:\n",
    "                category = 'unknown'\n",
    "\n",
    "            tr_range = get_fixed_length_tr_range([row['imagine_task.started']], TR)[0]\n",
    "            imagine_category_blocks[category].append(tr_range)\n",
    "\n",
    "    return {\n",
    "        'view': image_to_view_TRs,                # dict: image → TRs\n",
    "        'recall': image_to_recall_TRs,            # dict: image → TRs\n",
    "        'imagine': dict(imagine_category_blocks)  # dict: category → list of TR lists\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for subject 1:\n",
    "subj_behavior_file = {1:\"psychopy_data/1_fmri design_2025-03-04_15h31.40.655.csv\",\n",
    "                      3:\"psychopy_data/2_fmri design_2025-03-05_15h36.18.659.csv\",\n",
    "                      4:\"psychopy_data/4_fmri design_2025-03-05_14h28.37.417.csv\"\n",
    "                      }\n",
    "results_sub01 = convert_psychopy_time_to_fmri_index(subj_behavior_file[1], 1, TR=1.5)\n",
    "results_sub03 = convert_psychopy_time_to_fmri_index(subj_behavior_file[3], 2, TR=1.5)\n",
    "results_sub04 = convert_psychopy_time_to_fmri_index(subj_behavior_file[4], 4, TR=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog1.jpg': [15, 16, 17, 18, 19, 20, 21],\n",
       " 'dog2.jpg': [32, 33, 34, 35, 36, 37, 38],\n",
       " 'dog3.jpg': [49, 50, 51, 52, 53, 54, 55],\n",
       " 'dog4.jpg': [67, 68, 69, 70, 71, 72, 73],\n",
       " 'dog5.jpg': [99, 100, 101, 102, 103, 104, 105],\n",
       " 'dog6.jpg': [117, 118, 119, 120, 121, 122, 123],\n",
       " 'dog7.jpg': [134, 135, 136, 137, 138, 139, 140],\n",
       " 'dog8.jpg': [151, 152, 153, 154, 155, 156, 157],\n",
       " 'dog9.jpg': [184, 185, 186, 187, 188, 189, 190],\n",
       " 'dog10.jpg': [201, 202, 203, 204, 205, 206, 207],\n",
       " 'dog11.jpg': [219, 220, 221, 222, 223, 224, 225],\n",
       " 'dog12.jpg': [236, 237, 238, 239, 240, 241, 242],\n",
       " 'sunflower1.jpg': [269, 270, 271, 272, 273, 274, 275],\n",
       " 'sunflower2.jpg': [286, 287, 288, 289, 290, 291, 292],\n",
       " 'sunflower3.jpg': [303, 304, 305, 306, 307, 308, 309],\n",
       " 'sunflower4.jpg': [321, 322, 323, 324, 325, 326, 327],\n",
       " 'sunflower5.jpg': [353, 354, 355, 356, 357, 358, 359],\n",
       " 'sunflower6.jpg': [371, 372, 373, 374, 375, 376, 377],\n",
       " 'sunflower7.jpg': [388, 389, 390, 391, 392, 393, 394],\n",
       " 'sunflower8.jpg': [405, 406, 407, 408, 409, 410, 411],\n",
       " 'sunflower9.jpg': [438, 439, 440, 441, 442, 443, 444],\n",
       " 'sunflower10.jpg': [455, 456, 457, 458, 459, 460, 461],\n",
       " 'sunflower11.jpg': [472, 473, 474, 475, 476, 477, 478],\n",
       " 'sunflower12.jpg': [490, 491, 492, 493, 494, 495, 496]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog1.jpg': [23, 24, 25, 26, 27, 28, 29],\n",
       " 'dog2.jpg': [41, 42, 43, 44, 45, 46, 47],\n",
       " 'dog3.jpg': [58, 59, 60, 61, 62, 63, 64],\n",
       " 'dog4.jpg': [75, 76, 77, 78, 79, 80, 81],\n",
       " 'dog5.jpg': [108, 109, 110, 111, 112, 113, 114],\n",
       " 'dog6.jpg': [125, 126, 127, 128, 129, 130, 131],\n",
       " 'dog7.jpg': [143, 144, 145, 146, 147, 148, 149],\n",
       " 'dog8.jpg': [160, 161, 162, 163, 164, 165, 166],\n",
       " 'dog9.jpg': [193, 194, 195, 196, 197, 198, 199],\n",
       " 'dog10.jpg': [210, 211, 212, 213, 214, 215, 216],\n",
       " 'dog11.jpg': [227, 228, 229, 230, 231, 232, 233],\n",
       " 'dog12.jpg': [245, 246, 247, 248, 249, 250, 251],\n",
       " 'sunflower1.jpg': [277, 278, 279, 280, 281, 282, 283],\n",
       " 'sunflower2.jpg': [295, 296, 297, 298, 299, 300, 301],\n",
       " 'sunflower3.jpg': [312, 313, 314, 315, 316, 317, 318],\n",
       " 'sunflower4.jpg': [329, 330, 331, 332, 333, 334, 335],\n",
       " 'sunflower5.jpg': [362, 363, 364, 365, 366, 367, 368],\n",
       " 'sunflower6.jpg': [379, 380, 381, 382, 383, 384, 385],\n",
       " 'sunflower7.jpg': [397, 398, 399, 400, 401, 402, 403],\n",
       " 'sunflower8.jpg': [414, 415, 416, 417, 418, 419, 420],\n",
       " 'sunflower9.jpg': [446, 447, 448, 449, 450, 451, 452],\n",
       " 'sunflower10.jpg': [464, 465, 466, 467, 468, 469, 470],\n",
       " 'sunflower11.jpg': [481, 482, 483, 484, 485, 486, 487],\n",
       " 'sunflower12.jpg': [498, 499, 500, 501, 502, 503, 504]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': [[89, 90, 91, 92, 93, 94, 95],\n",
       "  [174, 175, 176, 177, 178, 179, 180],\n",
       "  [259, 260, 261, 262, 263, 264, 265]],\n",
       " 'sunflower': [[343, 344, 345, 346, 347, 348, 349],\n",
       "  [428, 429, 430, 431, 432, 433, 434],\n",
       "  [512, 513, 514, 515, 516, 517, 518]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['imagine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = \"/jukebox/hasson/snastase/neu502b-2025/neu502b-fmri/data/bids/derivatives/fmriprep\" \n",
    "subjects = [\"sub-01/func/\", \"sub-03/func/\", \"sub-04/func/\"]  # Assuming 20 subjects, update as needed\n",
    "task_prefix = \"imagine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"neural_activity\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "task_prefix = \"imagine\"\n",
    "space_tag = \"space-MNI152NLin2009cAsym\"  # standard space\n",
    "\n",
    "def build_intersection_mask(subject_list, data_dir, space_tag=\"space-MNI152NLin2009cAsym\", task_prefix=\"imagine\"):\n",
    "    mask_paths = []\n",
    "    for subject in subject_list:\n",
    "        subject_path = os.path.join(data_dir, subject)\n",
    "        for f in os.listdir(subject_path):\n",
    "            if (task_prefix in f) and (space_tag in f) and (\"desc-brain_mask.nii.gz\" in f):\n",
    "                mask_paths.append(os.path.join(subject_path, f))\n",
    "                break\n",
    "    print(f\"Found {len(mask_paths)} masks\")\n",
    "    intersected = intersect_masks(mask_paths, threshold=1.0, connected=False)\n",
    "    return intersected\n",
    "\n",
    "def extract_neural_activity(subject, group_mask_img):\n",
    "    print(f\"Processing {subject}...\")\n",
    "\n",
    "    # Find BOLD\n",
    "    bold_file = None\n",
    "    for f in os.listdir(os.path.join(data_dir, subject)):\n",
    "        if (task_prefix in f) and (space_tag in f) and (\"desc-preproc_bold.nii.gz\" in f):\n",
    "            bold_file = os.path.join(data_dir, subject, f)\n",
    "            break\n",
    "\n",
    "    if not bold_file:\n",
    "        print(f\"⚠️ No MNI-space BOLD file for {subject}\")\n",
    "        return None\n",
    "\n",
    "    bold_img = nib.load(bold_file)\n",
    "\n",
    "    # Load confounds (optional)\n",
    "    confound_file = None\n",
    "    for f in os.listdir(os.path.join(data_dir, subject)):\n",
    "        if (task_prefix in f) and (\"desc-confounds_timeseries.tsv\" in f):\n",
    "            confound_file = os.path.join(data_dir, subject, f)\n",
    "            break\n",
    "\n",
    "    confounds = None\n",
    "    if confound_file:\n",
    "        conf_df = pd.read_csv(confound_file, sep=\"\\t\")\n",
    "        nuisance_cols = ['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']\n",
    "        nuisance_cols = [col for col in nuisance_cols if col in conf_df.columns]\n",
    "        confounds = conf_df[nuisance_cols].fillna(method='bfill').fillna(method='ffill').values\n",
    "\n",
    "    # Extract data using the shared mask\n",
    "    masker = NiftiMasker(mask_img=group_mask_img, standardize=True, high_pass=0.01, t_r=1.5)\n",
    "    masker.fit(bold_img)\n",
    "    time_series = masker.transform(bold_img, confounds=confounds)\n",
    "\n",
    "    print(f\"Extracted neural activity shape: {time_series.shape}\")\n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 masks\n",
      "Processing sub-01/func/...\n",
      "Extracted neural activity shape: (535, 70482)\n",
      "Processing sub-03/func/...\n",
      "Extracted neural activity shape: (535, 70482)\n",
      "Processing sub-04/func/...\n",
      "Extracted neural activity shape: (535, 70482)\n"
     ]
    }
   ],
   "source": [
    "# Build shared group-level mask\n",
    "group_mask = build_intersection_mask(subjects, data_dir)\n",
    "\n",
    "# Extract aligned time series for all subjects\n",
    "ts_sub01 = extract_neural_activity(subjects[0], group_mask)\n",
    "ts_sub03 = extract_neural_activity(subjects[1], group_mask)\n",
    "ts_sub04 = extract_neural_activity(subjects[2], group_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate pattern analysis (whole brain analysis)\n",
    "* Train the binary category classifier on the view condition\n",
    "* Test it on the recall and imagine condition (also cross validation)\n",
    "\n",
    "See notebook: fmri-4/fmri-4-mvpa-key.ipynb\n",
    "\n",
    "Ref handbook: https://brainhack-princeton.github.io/handbook/content_pages/05-02-mvpa.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tr_dict, category_labels, time_series):\n",
    "    \"\"\"\n",
    "    tr_dict: dict of image → list of TRs\n",
    "    category_labels: dict of image → 0 or 1\n",
    "    time_series: (T, V) matrix\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, trs in tr_dict.items():\n",
    "        if img not in category_labels:\n",
    "            continue\n",
    "        label = category_labels[img]\n",
    "        for tr in trs:\n",
    "            if tr < time_series.shape[0]:\n",
    "                X.append(time_series[tr])\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_blockwise_data(tr_dict, category_labels, time_series):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, trs in tr_dict.items():\n",
    "        if img not in category_labels:\n",
    "            continue\n",
    "        label = category_labels[img]\n",
    "        block_data = [time_series[tr] for tr in trs if tr < time_series.shape[0]]\n",
    "        if block_data:\n",
    "            X.append(np.mean(block_data, axis=0))  # average over TRs\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_imagine_data(imagine_dict, time_series, average_blocks=True):\n",
    "    X, y = [], []\n",
    "    for cat, blocks in imagine_dict.items():\n",
    "        label = 0 if cat == 'dog' else 1\n",
    "        for block in blocks:\n",
    "            block_data = [time_series[tr] for tr in block if tr < time_series.shape[0]]\n",
    "            if not block_data: continue\n",
    "            if average_blocks:\n",
    "                X.append(np.mean(block_data, axis=0))\n",
    "                y.append(label)\n",
    "            else:\n",
    "                for tr_data in block_data:\n",
    "                    X.append(tr_data)\n",
    "                    y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mvpa_pipeline_multi_subject(\n",
    "    time_series_list, results_list,\n",
    "    blockwise=True, apply_pca=True, model_type='logreg',\n",
    "    n_components=100, average_proba_blocks=False\n",
    "):\n",
    "    from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.utils import shuffle\n",
    "    import numpy as np\n",
    "\n",
    "    # === Pool training data from all subjects ===\n",
    "    X_train_all, y_train_all = [], []\n",
    "\n",
    "    for ts, results in zip(time_series_list, results_list):\n",
    "        label_dict = {img: 0 if 'dog' in img else 1 for img in results['view'].keys()}\n",
    "        if blockwise:\n",
    "            Xi, yi = prepare_blockwise_data(results['view'], label_dict, ts)\n",
    "        else:\n",
    "            Xi, yi = prepare_data(results['view'], label_dict, ts)\n",
    "        X_train_all.append(Xi)\n",
    "        y_train_all.append(yi)\n",
    "\n",
    "    X_train = np.concatenate(X_train_all)\n",
    "    y_train = np.concatenate(y_train_all)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    print(f\"Pooled train: {X_train.shape}\")\n",
    "\n",
    "    # === PCA ===\n",
    "    if apply_pca:\n",
    "        pca = PCA(n_components=min(n_components, X_train.shape[0], X_train.shape[1]))\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "    else:\n",
    "        pca = None\n",
    "\n",
    "    # === Classifier ===\n",
    "    if model_type == 'logreg':\n",
    "        clf = LogisticRegression(penalty='l2', solver='liblinear', C=1.0, max_iter=1000)\n",
    "    elif model_type == 'ridge':\n",
    "        clf = RidgeClassifier(alpha=1.0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # === Train and evaluate on pooled training set ===\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    print(f\"CV accuracy (view): {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    print(f\"Training accuracy (view): {train_acc:.2f}\")\n",
    "\n",
    "    # === Evaluate on recall and imagine for each subject separately ===\n",
    "    for i, (ts, results) in enumerate(zip(time_series_list, results_list), start=1):\n",
    "        label_dict = {img: 0 if 'dog' in img else 1 for img in results['view'].keys()}\n",
    "        print(f\"\\n--- Testing on subject {i:02d} ---\")\n",
    "\n",
    "        # ----- Recall -----\n",
    "        if average_proba_blocks:\n",
    "            y_recall, recall_preds = [], []\n",
    "            for img, trs in results['recall'].items():\n",
    "                if img not in label_dict:\n",
    "                    continue\n",
    "                label = label_dict[img]\n",
    "                block_data = [ts[tr] for tr in trs if tr < ts.shape[0]]\n",
    "                block_array = np.array(block_data)\n",
    "                if apply_pca:\n",
    "                    block_array = pca.transform(block_array)\n",
    "                probs = clf.predict_proba(block_array)\n",
    "                avg_prob = probs.mean(axis=0)\n",
    "                pred = np.argmax(avg_prob)\n",
    "                y_recall.append(label)\n",
    "                recall_preds.append(pred)\n",
    "            acc_recall = accuracy_score(y_recall, recall_preds)\n",
    "        else:\n",
    "            X_recall, y_recall = prepare_data(results['recall'], label_dict, ts)\n",
    "            if apply_pca:\n",
    "                X_recall = pca.transform(X_recall)\n",
    "            acc_recall = accuracy_score(y_recall, clf.predict(X_recall))\n",
    "        print(f\"Recall decoding accuracy: {acc_recall:.2f}\")\n",
    "\n",
    "        # ----- Imagine -----\n",
    "        if average_proba_blocks:\n",
    "            y_imagine, imagine_preds = [], []\n",
    "            for cat, blocks in results['imagine'].items():\n",
    "                label = 0 if cat == 'dog' else 1\n",
    "                for trs in blocks:\n",
    "                    block_data = [ts[tr] for tr in trs if tr < ts.shape[0]]\n",
    "                    block_array = np.array(block_data)\n",
    "                    if apply_pca:\n",
    "                        block_array = pca.transform(block_array)\n",
    "                    probs = clf.predict_proba(block_array)\n",
    "                    avg_prob = probs.mean(axis=0)\n",
    "                    pred = np.argmax(avg_prob)\n",
    "                    y_imagine.append(label)\n",
    "                    imagine_preds.append(pred)\n",
    "            acc_imagine = accuracy_score(y_imagine, imagine_preds)\n",
    "        else:\n",
    "            X_imagine, y_imagine = prepare_imagine_data(results['imagine'], ts, average_blocks=blockwise)\n",
    "            if apply_pca:\n",
    "                X_imagine = pca.transform(X_imagine)\n",
    "            acc_imagine = accuracy_score(y_imagine, clf.predict(X_imagine))\n",
    "        print(f\"Imagine decoding accuracy: {acc_imagine:.2f}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_labels(view_dict):\n",
    "    return {img: 0 if 'dog' in img.lower() else 1 for img in view_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled train: (504, 70482)\n",
      "CV accuracy (view): 0.50 ± 0.03\n",
      "Training accuracy (view): 0.56\n",
      "\n",
      "--- Testing on subject 01 ---\n",
      "Recall decoding accuracy: 0.49\n",
      "Imagine decoding accuracy: 0.50\n",
      "\n",
      "--- Testing on subject 02 ---\n",
      "Recall decoding accuracy: 0.49\n",
      "Imagine decoding accuracy: 0.52\n",
      "\n",
      "--- Testing on subject 03 ---\n",
      "Recall decoding accuracy: 0.56\n",
      "Imagine decoding accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "clf = run_mvpa_pipeline_multi_subject(\n",
    "    time_series_list=[ts_sub01, ts_sub03, ts_sub04],\n",
    "    results_list=[results_sub01, results_sub03, results_sub04],\n",
    "    blockwise=False,\n",
    "    apply_pca=True,\n",
    "    model_type='ridge',  # or 'ridge'\n",
    "    n_components=10,\n",
    "    average_proba_blocks=False  # optional smoothing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representational Similarity Analysis (RSA) \n",
    "\n",
    "* Keep the category the same (e.g. dog), what is the similarity between view, recall, and imagine conditions\n",
    "* Compare the within-category similarity (within dogs vs within flowers)\n",
    "* Expect flower category to be more clustered because of visual similarity\n",
    "* Produce correlation matrix\n",
    "\n",
    "See notebook: fmri-5/fmri-5-rsa-key.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
