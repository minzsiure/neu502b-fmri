{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nibabel nilearn scikit-learn pandas numpy matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PsychPY timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert start/stop times to scan indices\n",
    "def time_to_index(times, TR=1.5):\n",
    "    return np.round(np.array(times) / TR).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_length_tr_range(start_times, TR=1.5, duration_sec=10):\n",
    "    \"\"\"\n",
    "    Convert a list of start times to TR indices of fixed duration.\n",
    "    Returns a list of lists (one TR range per event).\n",
    "    \"\"\"\n",
    "    n_TRs = int(np.round(duration_sec / TR))  # e.g. 10s / 1.5s = 6.67 → 7\n",
    "    indices = []\n",
    "    for start in start_times:\n",
    "        start_idx = int(np.round(start / TR))\n",
    "        indices.append(list(range(start_idx, start_idx + n_TRs)))\n",
    "    return indices\n",
    "    \n",
    "def convert_psychopy_time_to_fmri_index(beh_file, subject_id, TR=1.5):\n",
    "    df = pd.read_csv(beh_file)\n",
    "    picture_col = f'picture_{subject_id}'\n",
    "\n",
    "    image_to_view_TRs = {}\n",
    "    image_to_recall_TRs = {}\n",
    "    imagine_category_blocks = defaultdict(list)  # 'dog' → list of TR lists, etc.\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        image_name = row.get(picture_col)\n",
    "        if pd.isna(image_name):\n",
    "            continue\n",
    "\n",
    "        # VIEW (fixed length)\n",
    "        if not pd.isna(row.get('view.started')):\n",
    "            tr_range = get_fixed_length_tr_range([row['view.started']], TR)[0]\n",
    "            image_to_view_TRs[image_name] = tr_range\n",
    "\n",
    "        # RECALL (fixed length)\n",
    "        if not pd.isna(row.get('recall.started')):\n",
    "            tr_range = get_fixed_length_tr_range([row['recall.started']], TR)[0]\n",
    "            image_to_recall_TRs[image_name] = tr_range\n",
    "\n",
    "    # IMAGINE (grouped by category)\n",
    "    if 'imagine_task.started' in df.columns:\n",
    "        valid_rows = df[df['imagine_task.started'].notna()]\n",
    "        for _, row in valid_rows.iterrows():\n",
    "            image_name = row.get(picture_col)\n",
    "            if pd.isna(image_name):\n",
    "                continue\n",
    "\n",
    "            # Infer category\n",
    "            if 'dog' in image_name.lower():\n",
    "                category = 'dog'\n",
    "            elif 'sunflower' in image_name.lower():\n",
    "                category = 'sunflower'\n",
    "            else:\n",
    "                category = 'unknown'\n",
    "\n",
    "            tr_range = get_fixed_length_tr_range([row['imagine_task.started']], TR)[0]\n",
    "            imagine_category_blocks[category].append(tr_range)\n",
    "\n",
    "    return {\n",
    "        'view': image_to_view_TRs,                # dict: image → TRs\n",
    "        'recall': image_to_recall_TRs,            # dict: image → TRs\n",
    "        'imagine': dict(imagine_category_blocks)  # dict: category → list of TR lists\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for subject 1:\n",
    "subj_behavior_file = {1:\"psychopy_data/1_fmri design_2025-03-04_15h31.40.655.csv\",\n",
    "                      3:\"psychopy_data/2_fmri design_2025-03-05_15h36.18.659.csv\",\n",
    "                      4:\"psychopy_data/4_fmri design_2025-03-05_14h28.37.417.csv\"\n",
    "                      }\n",
    "results_sub01 = convert_psychopy_time_to_fmri_index(subj_behavior_file[1], 1, TR=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog1.jpg': [15, 16, 17, 18, 19, 20, 21],\n",
       " 'dog2.jpg': [32, 33, 34, 35, 36, 37, 38],\n",
       " 'dog3.jpg': [49, 50, 51, 52, 53, 54, 55],\n",
       " 'dog4.jpg': [67, 68, 69, 70, 71, 72, 73],\n",
       " 'dog5.jpg': [99, 100, 101, 102, 103, 104, 105],\n",
       " 'dog6.jpg': [117, 118, 119, 120, 121, 122, 123],\n",
       " 'dog7.jpg': [134, 135, 136, 137, 138, 139, 140],\n",
       " 'dog8.jpg': [151, 152, 153, 154, 155, 156, 157],\n",
       " 'dog9.jpg': [184, 185, 186, 187, 188, 189, 190],\n",
       " 'dog10.jpg': [201, 202, 203, 204, 205, 206, 207],\n",
       " 'dog11.jpg': [219, 220, 221, 222, 223, 224, 225],\n",
       " 'dog12.jpg': [236, 237, 238, 239, 240, 241, 242],\n",
       " 'sunflower1.jpg': [269, 270, 271, 272, 273, 274, 275],\n",
       " 'sunflower2.jpg': [286, 287, 288, 289, 290, 291, 292],\n",
       " 'sunflower3.jpg': [303, 304, 305, 306, 307, 308, 309],\n",
       " 'sunflower4.jpg': [321, 322, 323, 324, 325, 326, 327],\n",
       " 'sunflower5.jpg': [353, 354, 355, 356, 357, 358, 359],\n",
       " 'sunflower6.jpg': [371, 372, 373, 374, 375, 376, 377],\n",
       " 'sunflower7.jpg': [388, 389, 390, 391, 392, 393, 394],\n",
       " 'sunflower8.jpg': [405, 406, 407, 408, 409, 410, 411],\n",
       " 'sunflower9.jpg': [438, 439, 440, 441, 442, 443, 444],\n",
       " 'sunflower10.jpg': [455, 456, 457, 458, 459, 460, 461],\n",
       " 'sunflower11.jpg': [472, 473, 474, 475, 476, 477, 478],\n",
       " 'sunflower12.jpg': [490, 491, 492, 493, 494, 495, 496]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog1.jpg': [23, 24, 25, 26, 27, 28, 29],\n",
       " 'dog2.jpg': [41, 42, 43, 44, 45, 46, 47],\n",
       " 'dog3.jpg': [58, 59, 60, 61, 62, 63, 64],\n",
       " 'dog4.jpg': [75, 76, 77, 78, 79, 80, 81],\n",
       " 'dog5.jpg': [108, 109, 110, 111, 112, 113, 114],\n",
       " 'dog6.jpg': [125, 126, 127, 128, 129, 130, 131],\n",
       " 'dog7.jpg': [143, 144, 145, 146, 147, 148, 149],\n",
       " 'dog8.jpg': [160, 161, 162, 163, 164, 165, 166],\n",
       " 'dog9.jpg': [193, 194, 195, 196, 197, 198, 199],\n",
       " 'dog10.jpg': [210, 211, 212, 213, 214, 215, 216],\n",
       " 'dog11.jpg': [227, 228, 229, 230, 231, 232, 233],\n",
       " 'dog12.jpg': [245, 246, 247, 248, 249, 250, 251],\n",
       " 'sunflower1.jpg': [277, 278, 279, 280, 281, 282, 283],\n",
       " 'sunflower2.jpg': [295, 296, 297, 298, 299, 300, 301],\n",
       " 'sunflower3.jpg': [312, 313, 314, 315, 316, 317, 318],\n",
       " 'sunflower4.jpg': [329, 330, 331, 332, 333, 334, 335],\n",
       " 'sunflower5.jpg': [362, 363, 364, 365, 366, 367, 368],\n",
       " 'sunflower6.jpg': [379, 380, 381, 382, 383, 384, 385],\n",
       " 'sunflower7.jpg': [397, 398, 399, 400, 401, 402, 403],\n",
       " 'sunflower8.jpg': [414, 415, 416, 417, 418, 419, 420],\n",
       " 'sunflower9.jpg': [446, 447, 448, 449, 450, 451, 452],\n",
       " 'sunflower10.jpg': [464, 465, 466, 467, 468, 469, 470],\n",
       " 'sunflower11.jpg': [481, 482, 483, 484, 485, 486, 487],\n",
       " 'sunflower12.jpg': [498, 499, 500, 501, 502, 503, 504]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': [[89, 90, 91, 92, 93, 94, 95],\n",
       "  [174, 175, 176, 177, 178, 179, 180],\n",
       "  [259, 260, 261, 262, 263, 264, 265]],\n",
       " 'sunflower': [[343, 344, 345, 346, 347, 348, 349],\n",
       "  [428, 429, 430, 431, 432, 433, 434],\n",
       "  [512, 513, 514, 515, 516, 517, 518]]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sub01['imagine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = \"/jukebox/hasson/snastase/neu502b-2025/neu502b-fmri/data/bids/derivatives/fmriprep\" \n",
    "subjects = [\"sub-01/func/\", \"sub-03/func/\", \"sub-04/func/\"]  # Assuming 20 subjects, update as needed\n",
    "task_prefix = \"imagine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"neural_activity\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def extract_neural_activity(subject):\n",
    "    print(f\"Processing {subject}...\")\n",
    "\n",
    "    # Find the preprocessed BOLD image\n",
    "    bold_file = None\n",
    "    for f in os.listdir(os.path.join(data_dir, subject)):\n",
    "        if task_prefix in f and \"desc-preproc_bold.nii.gz\" in f:\n",
    "            bold_file = os.path.join(data_dir, subject, f)\n",
    "            break\n",
    "\n",
    "    if not bold_file:\n",
    "        print(f\"No preprocessed BOLD file found for {subject}.\")\n",
    "        return\n",
    "\n",
    "    # Load fMRI data\n",
    "    bold_img = nib.load(bold_file)\n",
    "\n",
    "    # Find brain mask\n",
    "    mask_file = None\n",
    "    for f in os.listdir(os.path.join(data_dir, subject)):\n",
    "        if task_prefix in f and \"desc-brain_mask.nii.gz\" in f:\n",
    "            mask_file = os.path.join(data_dir, subject, f)\n",
    "            break\n",
    "\n",
    "    if mask_file:\n",
    "        mask_img = nib.load(mask_file)\n",
    "        print(f\"Using brain mask: {mask_file}\")\n",
    "    else:\n",
    "        print(f\"No explicit brain mask found. Will compute one.\")\n",
    "        mask_img = None\n",
    "    \n",
    "    confound_tsv = None\n",
    "    for f in os.listdir(os.path.join(data_dir, subject)):\n",
    "        if (task_prefix in f) and (\"desc-confounds_timeseries.tsv\" in f):\n",
    "            confound_tsv = os.path.join(data_dir, f)\n",
    "            break\n",
    "\n",
    "    confound_data = None\n",
    "    if confound_tsv is not None:\n",
    "        df_conf = pd.read_csv(confound_tsv, sep='\\t')\n",
    "        # pick some columns, e.g., 6 motion parameters\n",
    "        nuisance_cols = ['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']\n",
    "        nuisance_cols = [c for c in nuisance_cols if c in df_conf.columns]\n",
    "        confound_data = df_conf[nuisance_cols].fillna(method='bfill').fillna(method='ffill').values\n",
    "\n",
    "# ---------\n",
    "    \n",
    "    # Extract time series from masked brain voxels\n",
    "    masker = NiftiMasker(mask_img=mask_img, \n",
    "                         standardize=True,\n",
    "                        high_pass=0.01)\n",
    "    masker.fit(bold_img)\n",
    "    time_series = masker.transform(bold_img, confounds=confound_data)\n",
    "    \n",
    "    print(f\"Extracted neural activity shape (TRs x voxels): {time_series.shape}\")  # (T, N_voxels)\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sub-01/func/...\n",
      "Using brain mask: /jukebox/hasson/snastase/neu502b-2025/neu502b-fmri/data/bids/derivatives/fmriprep/sub-01/func/sub-01_task-imagine_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/yx7967/miniconda3/envs/myenv/lib/python3.8/site-packages/nilearn/maskers/nifti_masker.py:110: UserWarning: imgs are being resampled to the mask_img resolution. This process is memory intensive. You might want to provide a target_affine that is equal to the affine of the imgs or resample the mask beforehand to save memory and computation time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted neural activity shape: (535, 73291)\n",
      "Processing sub-03/func/...\n",
      "Using brain mask: /jukebox/hasson/snastase/neu502b-2025/neu502b-fmri/data/bids/derivatives/fmriprep/sub-03/func/sub-03_task-imagine_space-T1w_desc-brain_mask.nii.gz\n",
      "Extracted neural activity shape: (535, 60764)\n",
      "Processing sub-04/func/...\n",
      "Using brain mask: /jukebox/hasson/snastase/neu502b-2025/neu502b-fmri/data/bids/derivatives/fmriprep/sub-04/func/sub-04_task-imagine_space-T1w_desc-brain_mask.nii.gz\n",
      "Extracted neural activity shape: (535, 56978)\n"
     ]
    }
   ],
   "source": [
    "# Run processing for all subjects\n",
    "for subject in subjects:\n",
    "    extract_neural_activity(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate pattern analysis (whole brain analysis)\n",
    "* Train the binary category classifier on the view condition\n",
    "* Test it on the recall and imagine condition (also cross validation)\n",
    "\n",
    "See notebook: fmri-4/fmri-4-mvpa-key.ipynb\n",
    "\n",
    "Ref handbook: https://brainhack-princeton.github.io/handbook/content_pages/05-02-mvpa.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representational Similarity Analysis (RSA) \n",
    "\n",
    "* Keep the category the same (e.g. dog), what is the similarity between view, recall, and imagine conditions\n",
    "* Compare the within-category similarity (within dogs vs within flowers)\n",
    "* Expect flower category to be more clustered because of visual similarity\n",
    "* Produce correlation matrix\n",
    "\n",
    "See notebook: fmri-5/fmri-5-rsa-key.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
